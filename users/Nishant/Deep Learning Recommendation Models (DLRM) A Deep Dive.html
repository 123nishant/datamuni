<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Deep Learning Recommendation Models (DLRM): A Deep Dive</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Deep Learning Recommendation Models (DLRM): A Deep Dive</h1>
</header>
<section data-field="subtitle" class="p-summary">
In 21st century the currency is not Data. It’s the Attention of People.
</section>
<section data-field="body" class="e-content">
<section name="da10" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1731" id="1731" class="graf graf--h3 graf--leading graf--title">Deep Learning Recommendation Models (DLRM): A Deep&nbsp;Dive</h3><h3 name="e2d0" id="e2d0" class="graf graf--h3 graf-after--h3"><strong class="markup--strong markup--h3-strong">In the 21st century the currency is not Data. It’s the Attention of&nbsp;People.</strong></h3><p name="d303" id="d303" class="graf graf--p graf-after--h3">Recommendation systems are built to predict what users might like, especially when there are lots of choices available.</p><p name="ff23" id="ff23" class="graf graf--p graf-after--p">This
 post gives a deep dive into the architecture and issues experienced 
during the deployment of DLRM model. This algorithm was open-sourced by 
Facebook on 31st March 2019.</p><p name="7268" id="7268" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">It’s a part of the popular MLPerf Benchmark.</em></strong></p><figure name="2a0f" id="2a0f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*rlnAH_rod_BKr3cP" data-width="526" data-height="295" data-is-featured="true" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0rlnAH_rod_BKr3cP.png"><figcaption class="imageCaption"><a href="https://arxiv.org/pdf/1906.00091.pdf" data-href="https://arxiv.org/pdf/1906.00091.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">DEEP LEARNING RECOMMENDATION MODEL ARCHITECTURE ( DLRM&nbsp;)</a></figcaption></figure><blockquote name="e29c" id="e29c" class="graf graf--blockquote graf-after--figure"><strong class="markup--strong markup--blockquote-strong">Why should you consider using DLRM&nbsp;?</strong></blockquote><p name="5885" id="5885" class="graf graf--p graf-after--blockquote">This paper attempts to combine 2 important concepts that are driving the architectural changes in recommendation systems&nbsp;:</p><ol class="postList"><li name="f552" id="f552" class="graf graf--li graf-after--p">From
 the view of Recommendation Systems, initially content filtering systems
 was employed which matched users to products based on their 
preferences. This subsequently evolved to use collaborative filtering 
where recommendations were based on past user behaviors.</li><li name="3f1e" id="3f1e" class="graf graf--li graf-after--li">From
 the view of Predictive Analytics, it relies on statistical models to 
classify or predict probability of events based on the given data. These
 models shifted from simple models such as linear and logistic 
regression to models that incorporate deep networks.</li></ol><p name="3dff" id="3dff" class="graf graf--p graf-after--li">In this paper, the authors claim to succeed in unifying these 2 perspectives in the DLRM Model.</p><p name="60ec" id="60ec" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Few notable features&nbsp;:</strong></p><ol class="postList"><li name="5e70" id="5e70" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Extensive use of Embedding Tables&nbsp;</strong>: Embedding provide a rich and meaningful representation of the data of the users.</li><li name="87ac" id="87ac" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Exploits Multi-layer Perceptron (MLP): </strong>MLP presents a flavor of Deep Learning. They can well address the limitations presented by the statistical methods&nbsp;.</li><li name="7087" id="7087" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Model Parallelism&nbsp;: </strong>Poses a less overhead on memory, and speeds it up.</li><li name="9f7a" id="9f7a" class="graf graf--li graf-after--li graf--trailing"><strong class="markup--strong markup--li-strong">Interaction between Embeddings</strong>&nbsp;:
 Used to interpret latent factors (i.e. hidden factors) between feature 
interactions. An example would be how likely a user who likes comedy and
 horror movies would like a horror-comedy movie. Such interactions play a
 major role in working of recommendation systems.</li></ol></div></div></section><section name="15c4" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="a7d9" id="a7d9" class="graf graf--figure graf--leading"><img class="graf-image" data-image-id="0*maDJlm5iWkQ3VdyT" data-width="5999" data-height="2703" data-unsplash-photo-id="XIKBB9upCcI" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0maDJlm5iWkQ3VdyT.jpeg"></figure><h4 name="5b58" id="5b58" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">LET’S START</strong></h4><h3 name="307f" id="307f" class="graf graf--h3 graf-after--h4"><strong class="markup--strong markup--h3-strong">Model Workflow:</strong></h3><figure name="4906" id="4906" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*w1I1laI2NeeTEyUBiY9O4w.gif" data-width="900" data-height="506" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/1w1I1laI2NeeTEyUBiY9O4w.gif"><figcaption class="imageCaption">DLRM Workflow</figcaption></figure><ul class="postList"><li name="93ea" id="93ea" class="graf graf--li graf-after--figure">Model
 uses Embedding to process Sparse Features that represent Categorical 
Data and a Multi-layer Perceptron (MLP) to process dense features,</li><li name="2189" id="2189" class="graf graf--li graf-after--li">Interacts these features explicitly using the statistical techniques proposed&nbsp;.</li><li name="e373" id="e373" class="graf graf--li graf-after--li graf--trailing">Finally, it finds the event probability by post-processing the interactions with another MLP.</li></ul></div></div></section><section name="60c4" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3510" id="3510" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">ARCHITECTURE&nbsp;:</strong></h3><ol class="postList"><li name="0581" id="0581" class="graf graf--li graf-after--h3">Embeddings</li><li name="3958" id="3958" class="graf graf--li graf-after--li">Matrix Factorization</li><li name="b836" id="b836" class="graf graf--li graf-after--li">Factorization Machine</li><li name="51b3" id="51b3" class="graf graf--li graf-after--li">Multi-layer Perceptron (MLP)</li></ol><p name="4164" id="4164" class="graf graf--p graf-after--li">Let’s discuss them in a little detail.</p><ol class="postList"><li name="0d60" id="0d60" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Embeddings&nbsp;:</strong></li></ol><p name="6a6d" id="6a6d" class="graf graf--p graf-after--li"><em class="markup--em markup--p-em">Mapping of concepts, objects or items into a vector space is called an Embedding</em></p><p name="1188" id="1188" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Eg&nbsp;:</strong></p><figure name="8d3d" id="8d3d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*3Ym44dctxGISlqGQ" data-width="995" data-height="775" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/03Ym44dctxGISlqGQ.png"></figure><p name="96fe" id="96fe" class="graf graf--p graf-after--figure">In
 the context of neural networks, embeddings are low-dimensional&nbsp;, 
learned continuous vector representation of discrete variables.</p><p name="a6b8" id="a6b8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Why should we use Embeddings instead of other options such as lists of sparse items&nbsp;?</strong></p><ul class="postList"><li name="79f8" id="79f8" class="graf graf--li graf-after--p">Reduces dimensionality of categorical variables and meaningfully represent categories in the abstract space</li><li name="2719" id="2719" class="graf graf--li graf-after--li">We can measure distance between Embeddings in a more Meaningful way.</li><li name="11db" id="11db" class="graf graf--li graf-after--li">Embedding
 Elements represent sparse features in some abstract space relevant to 
the model at hand, while integers represent an ordering of the input 
data.</li><li name="611e" id="611e" class="graf graf--li graf-after--li">Embedding vectors project <strong class="markup--strong markup--li-strong">n dimensional items space </strong>into <strong class="markup--strong markup--li-strong">d dimensional embedding vectors </strong>where n &gt;&gt; d</li></ul><p name="212f" id="212f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Matrix Factorization&nbsp;:</strong></p><p name="ced0" id="ced0" class="graf graf--p graf-after--p">This technique belongs to a class of Collaborative filtering algorithms used in Recommendation Systems.</p><p name="3ee6" id="3ee6" class="graf graf--p graf-after--p">Matrix
 Factorization algorithms work by decomposing user-item interaction 
matrix into the product of 2 lower dimensionality rectangular matrices</p><figure name="edbb" id="edbb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Gs49CEBwLp6vYZsI" data-width="590" data-height="213" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0Gs49CEBwLp6vYZsI.png"></figure><p name="0382" id="0382" class="graf graf--p graf-after--figure"><em class="markup--em markup--p-em">Refer&nbsp;:</em><a href="https://developers.google.com/machine-learning/recommendation/collaborative/matrix" data-href="https://developers.google.com/machine-learning/recommendation/collaborative/matrix" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">https://developers.google.com/machine-learning/recommendation/collaborative/matrix</em></a><em class="markup--em markup--p-em"> for more details</em></p><p name="b2ee" id="b2ee" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Factorization Machines&nbsp;:</strong></p><p name="4640" id="4640" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Good choice for tasks dealing with high dimensional Sparse Datasets.</em></p><p name="8987" id="8987" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">FM is an improved version of MF</strong></p><p name="8b57" id="8b57" class="graf graf--p graf-after--p">It is designed to capture interactions between features within high dimensional sparse datasets economically.</p><figure name="1667" id="1667" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*-8Q3H2GSzt1eTRG9" data-width="670" data-height="271" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0-8Q3H2GSzt1eTRG9.png"><figcaption class="imageCaption">Factorization Matrix (FM)&nbsp;Equation</figcaption></figure><figure name="a544" id="a544" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="0*wt1NSe-hgtdJpxYj" data-width="551" data-height="279" alt="FM equation Workflow" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0wt1NSe-hgtdJpxYj.png"></figure><p name="1397" id="1397" class="graf graf--p graf-after--figure">Features of Factorization Machines (FM)&nbsp;:</p><ul class="postList"><li name="5db4" id="5db4" class="graf graf--li graf-after--p">Able to estimate interactions in sparse settings because they break independence of interaction by parameters by factoring them.</li><li name="169c" id="169c" class="graf graf--li graf-after--li">Incorporates 2nd order interactions into a linear model with categorical data by defining a model of the form.</li></ul><figure name="8d96" id="8d96" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*jI_TfRWbUw6oRGci" data-width="229" data-height="27" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0jI_TfRWbUw6oRGci.png"></figure><figure name="f7dc" id="f7dc" class="graf graf--figure graf--layoutOutsetLeft graf-after--figure"><img class="graf-image" data-image-id="0*tkFVjfYMiOKsEFce" data-width="312" data-height="416" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0tkFVjfYMiOKsEFce.png"><figcaption class="imageCaption">Different orders of interaction matrices</figcaption></figure><p name="5e19" id="5e19" class="graf graf--p graf-after--figure">FMs
 factorize 2nd order interaction matrix to its latent factors (or 
embedding vectors) as in matrix factorization, which more effectively 
handles sparse data.</p><p name="9b2a" id="9b2a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Significantly
 reduces complexity of 2nd order interactions by only capturing 
interactions between pairs of distinct embedding vectors, yielding 
linear computational complexity.</em></strong></p><p name="5b53" id="5b53" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Refer&nbsp;: </em><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" data-href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html</em></a></p><p name="a358" id="a358" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Multi-layer Perceptron (MLP)&nbsp;:</strong></p><p name="0433" id="0433" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Finally, a little flavor of Deep Learning.</em></p><p name="c3f2" id="c3f2" class="graf graf--p graf-after--p">A Multilayer Perceptron (MLP) is a class of Feed-Forward Artificial Neural Network.</p><figure name="b1c1" id="b1c1" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><img class="graf-image" data-image-id="1*UyndHD1FdTHsAaeid2fn3Q.gif" data-width="732" data-height="448" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/1UyndHD1FdTHsAaeid2fn3Q.gif"></figure><p name="03cf" id="03cf" class="graf graf--p graf-after--figure">An MLP consists of at least 3 layers of nodes&nbsp;:</p><ul class="postList"><li name="3d1e" id="3d1e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Input layer</strong></li><li name="2270" id="2270" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Hidden layer</strong></li><li name="db23" id="db23" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Output layer</strong></li></ul><p name="eb15" id="eb15" class="graf graf--p graf-after--li">Except for input nodes, each node is a neuron that uses a nonlinear activation function.</p><p name="a782" id="a782" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">MLP utilizes a supervised learning called Backpropagation for training.</em></p><figure name="7f8f" id="7f8f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*XmYBFUWTrWaTACK8" data-width="540" data-height="304" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0XmYBFUWTrWaTACK8.gif"></figure><p name="382d" id="382d" class="graf graf--p graf-after--figure">These methods have been used to capture more complex interactions.</p><p name="5a4a" id="5a4a" class="graf graf--p graf-after--p">MLPs with sufficient depth and width can fit data to arbitrary precision.</p><p name="6b6f" id="6b6f" class="graf graf--p graf-after--p">One specific case, <em class="markup--em markup--p-em">Neural
 Collaborative Filtering (NCF) used as part of MLPerf Benchmark, uses an
 MLP rather than dot product to compute interactions between embeddings 
in Matrix Factorization.</em></p><h3 name="6cb9" id="6cb9" class="graf graf--h3 graf-after--p">DLRM Operators by Framework</h3><figure name="4095" id="4095" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*KQpKYSBqt75H5JCZaFk1PA.png" data-width="753" data-height="180" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/1KQpKYSBqt75H5JCZaFk1PA.png"></figure><p name="507c" id="507c" class="graf graf--p graf-after--figure">You
 can find below the overall Architecture of open-source recommendation 
model system. All configurable parameters are outlined in blue. And <strong class="markup--strong markup--p-strong">the operators used are shown in Green.</strong></p><figure name="3e0a" id="3e0a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*O9r79-SKw1xsUbqf" data-width="646" data-height="296" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0O9r79-SKw1xsUbqf.png"><figcaption class="imageCaption">Source&nbsp;: <a href="https://arxiv.org/pdf/1906.03109.pdf" data-href="https://arxiv.org/pdf/1906.03109.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1906.03109.pdf</a></figcaption></figure><p name="1ee6" id="1ee6" class="graf graf--p graf-after--figure">We have 3 tested models from Facebook ( <a href="https://arxiv.org/pdf/1906.03109.pdf" data-href="https://arxiv.org/pdf/1906.03109.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Source&nbsp;: Architectural Implication of Facebook’s DNN-Based Personalized Recommendation</a>)</p><figure name="921e" id="921e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*DohG5SRSIqv0hYVF" data-width="651" data-height="167" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0DohG5SRSIqv0hYVF.png"></figure><p name="f990" id="f990" class="graf graf--p graf-after--figure graf--trailing">Model Architecture parameters are representative of production scale recommendation workloads for <strong class="markup--strong markup--p-strong">3 examples of recommendation models used, highlighting their diversity in terms of embedding table and FC sizes.</strong> Each parameter(column) is normalized to the smallest instance across all 3 configurations.</p></div></div></section><section name="7ba3" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="10d1" id="10d1" class="graf graf--h3 graf--leading"><strong class="markup--strong markup--h3-strong">ISSUES&nbsp;:</strong></h3><figure name="4933" id="4933" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*P7BuskcDAtf4AOGL" data-width="659" data-height="318" alt="Issues present with the DLRM Recommendation model&nbsp;: 1. Compute Dominated 2. Communication Dominated 3. Memory Bandwidth Domi" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0P7BuskcDAtf4AOGL.png"></figure><ol class="postList"><li name="dcd8" id="dcd8" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Memory Capacity Dominated</strong> ( Input from Network )</li><li name="31f1" id="31f1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Memory Band-Width Dominated</strong> ( Processing of Features&nbsp;: Embedding Lookup and MLP)</li><li name="3e46" id="3e46" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Communication Based</strong> ( Interaction between Features )</li><li name="ddae" id="ddae" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Compute Dominated </strong>( Compute/Run-Time Bottleneck)</li></ol><h3 name="c97e" id="c97e" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">1. MEMORY CAPACITY DOMINATED:</strong></h3><p name="fc82" id="fc82" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">(Input From Network)</strong></p><p name="59ed" id="59ed" class="graf graf--p graf-after--p">Source&nbsp;: <a href="https://arxiv.org/pdf/1906.00091.pdf" data-href="https://arxiv.org/pdf/1906.00091.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1906.00091.pdf</a></p><figure name="cc20" id="cc20" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Gd4cvFMV6jjP5uaB" data-width="634" data-height="250" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0Gd4cvFMV6jjP5uaB.png"></figure><p name="bbfd" id="bbfd" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">SOLUTION&nbsp;: </strong>Parallelism</p><p name="d486" id="d486" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">One of the basic and most important steps</em></p><ul class="postList"><li name="ece0" id="ece0" class="graf graf--li graf-after--p">Embeddings
 contribute the majority of parameters, with several tables each 
requiring excess of multiple GBs of memory. This necessitates 
Distribution of models across Multiple Devices.</li><li name="952f" id="952f" class="graf graf--li graf-after--li">MLP parameters are smaller in memory but translate to sizeable amounts of compute</li></ul><p name="23c0" id="23c0" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Data
 Parallelism is preferred for MLPs since this enables concurrent 
processing of samples on different devices and only requires 
communication when accumulating updates.</strong></p><h3 name="a7b5" id="a7b5" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Personalization&nbsp;:</strong></h3><p name="7cec" id="7cec" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">SETUP&nbsp;: </strong>Top
 MLP and interaction operator requires access to part of Mini-Batch from
 Bottom MLP and all of Embeddings. Since Model Parallelism has been used
 to distribute embeddings across devices, this requires a <strong class="markup--strong markup--p-strong">Personalized all-to-all communication.</strong></p><figure name="cf55" id="cf55" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Dq8lje6ErCINAsth" data-width="496" data-height="218" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0Dq8lje6ErCINAsth.png"><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Butterfly Shuffle for the all-to-all (Personalized) Communication&nbsp;. Source&nbsp;: </strong><a href="https://arxiv.org/pdf/1906.00091.pdf" data-href="https://arxiv.org/pdf/1906.00091.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1906.00091.pdf</a></figcaption></figure><p name="18b5" id="18b5" class="graf graf--p graf-after--figure">Slices (i.e. 1,2,3) are Embedding vectors that are supposed to be transferreed to target devices for personalization.</p><p name="4d0e" id="4d0e" class="graf graf--p graf-after--p">Currently transfers are only explicit copies</p><h3 name="c3ee" id="c3ee" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">2. MEMORY BANDWIDTH DOMINATED&nbsp;:</strong></h3><p name="2786" id="2786" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">(Processing of Features&nbsp;: Embedding Lookup and MLP)</strong></p><p name="e836" id="e836" class="graf graf--p graf-after--p">Source&nbsp;: <a href="https://arxiv.org/pdf/1909.02107.pdf" data-href="https://arxiv.org/pdf/1909.02107.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1909.02107.pdf</a></p><p name="e69e" id="e69e" class="graf graf--p graf-after--p"><a href="https://arxiv.org/pdf/1901.02103.pdf" data-href="https://arxiv.org/pdf/1901.02103.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1901.02103.pdf</a></p><figure name="6fe4" id="6fe4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*q675vLM0tOyNEjI6" data-width="960" data-height="540" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0q675vLM0tOyNEjI6.png"></figure><ul class="postList"><li name="457f" id="457f" class="graf graf--li graf-after--figure">MLP parameters are smaller in memory but translate to sizeable amounts of compute ( So issue will come during compute )</li><li name="6b0f" id="6b0f" class="graf graf--li graf-after--li">Embedding Lookups can cause memory constraints.</li></ul><p name="b9a6" id="b9a6" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">SOLUTION&nbsp;: </strong>Compositional Embeddings using Complementary Partitions</p><p name="5914" id="5914" class="graf graf--p graf-after--p">Representation of n items in d dimensional vector space can be broadly divided into 2 categories&nbsp;:</p><figure name="5a9c" id="5a9c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3m-qL-IdvmPK0Z_bY9g5Gg.png" data-width="636" data-height="486" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/13m-qL-IdvmPK0Z_bY9g5Gg.png"></figure><p name="4f44" id="4f44" class="graf graf--p graf-after--figure"><em class="markup--em markup--p-em">An
 approach is proposed for generating unique embedding for each 
categorical feature using Complementary Partitions of category set to 
generate Compositional Embeddings.</em></p><p name="bbc2" id="bbc2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Approaching Memory-Bandwidth Consumption issue:</strong></p><ol class="postList"><li name="897a" id="897a" class="graf graf--li graf-after--p">Hashing Trick</li><li name="15b2" id="15b2" class="graf graf--li graf-after--li">Quotient-Remainder Trick</li></ol><h4 name="b53e" id="b53e" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">HASHING TRICK&nbsp;:</strong></h4><p name="195d" id="195d" class="graf graf--p graf-after--h4">Naive approach of reducing embedding table using a simple Hash Function.</p><figure name="5f76" id="5f76" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*RK0j36Rsaj43QxUk" data-width="372" data-height="84" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0RK0j36Rsaj43QxUk.png"><figcaption class="imageCaption">Hashing Trick</figcaption></figure><p name="9f5b" id="9f5b" class="graf graf--p graf-after--figure">It significantly <strong class="markup--strong markup--p-strong">reduces the size of Embedding Matrix </strong>from <strong class="markup--strong markup--p-strong">O(|S|D) to O(mD)</strong> since m &lt;&lt; |S|.</p><p name="d649" id="d649" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Disadvantages&nbsp;:</strong></p><ul class="postList"><li name="52da" id="52da" class="graf graf--li graf-after--p">Does not yield a <em class="markup--em markup--li-em">Unique Embedding </em>for <em class="markup--em markup--li-em">each Category</em></li><li name="300b" id="300b" class="graf graf--li graf-after--li">Naively maps multiple categories to the same embedding vector</li><li name="3102" id="3102" class="graf graf--li graf-after--li">Results in <em class="markup--em markup--li-em">loss of Information,</em> hence, <em class="markup--em markup--li-em">rapid deterioration of model quality</em></li></ul><p name="1ba2" id="1ba2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">QUOTIENT-REMAINDER TRICK&nbsp;:</strong></p><p name="8a2e" id="8a2e" class="graf graf--p graf-after--p">Using
 2 complementary functions i.e. integer quotient and remainder 
functions&nbsp;: we can produce 2 separate embedding tables and combine 
them in a way that yields a unique embedding for each category.</p><p name="aaf0" id="aaf0" class="graf graf--p graf-after--p">It results in memory complexity <strong class="markup--strong markup--p-strong">O(D*|S|/m + mD)&nbsp;, a slight increase in memory compared to hashing trick&nbsp;,</strong></p><ul class="postList"><li name="7856" id="7856" class="graf graf--li graf-after--p">But with an added benefit of producing a unique representation.</li></ul><figure name="d6b1" id="d6b1" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*H_nsmRpcUxiI3U8g" data-width="372" data-height="118" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0H_nsmRpcUxiI3U8g.png"><figcaption class="imageCaption">Quotient-Remainder Trick</figcaption></figure><h3 name="2026" id="2026" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">COMPLEMENTARY PARTITIONS</strong></h3><p name="051b" id="051b" class="graf graf--p graf-after--h3">In
 the Quotient-Remainder trick, each operation partitions a set of 
categories in to “Multiple Buckets” such that every index in the same 
“bucket” is mapped to the same vector.</p><p name="1853" id="1853" class="graf graf--p graf-after--p">By combining embeddings from both quotient and remainder together, one is able to generate a distinct vector for each index.</p><p name="7717" id="7717" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">NOTE&nbsp;: Complementary Partitions&nbsp;: </strong>Avoids repetition of data or embedding tables across partitions (as it’s Complementary, duh&nbsp;!! )</p><h4 name="e09a" id="e09a" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Types Based on structure&nbsp;:</strong></h4><ul class="postList"><li name="e336" id="e336" class="graf graf--li graf-after--h4"><strong class="markup--strong markup--li-strong">Naive Complementary Partition</strong></li><li name="ec2e" id="ec2e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Quotient — Remainder Complementary Partitions</strong></li><li name="103e" id="103e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Generalized Quotient-Remainder Complementary Partitions</strong></li><li name="b36a" id="b36a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Chinese Remainder Partitions</strong></li></ul><h4 name="a96c" id="a96c" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">Types Based on Function&nbsp;:</strong></h4><figure name="d5de" id="d5de" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*z137OIgIHS8X3s34" data-width="960" data-height="540" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0z137OIgIHS8X3s34.png"><figcaption class="imageCaption">Types of Complementary Partitions based on&nbsp;function</figcaption></figure><blockquote name="ec51" id="ec51" class="graf graf--blockquote graf-after--figure"><strong class="markup--strong markup--blockquote-strong">Operation Based Compositional Embeddings&nbsp;:</strong></blockquote><p name="5d02" id="5d02" class="graf graf--p graf-after--blockquote">Assume that vectors in each embedding table are distinct&nbsp;. If <strong class="markup--strong markup--p-strong">concatenation </strong>operation is used, then compositional embeddings of any category are unique.</p><p name="3f05" id="3f05" class="graf graf--p graf-after--p">This approach reduces memory complexity of storing entire embedding table O(|S|D) to O(|P1|D1+|P2|D2+…|Pk|Dk).</p><p name="7cc0" id="7cc0" class="graf graf--p graf-after--p">Operation based embeddings are more complex due to the operations applied.</p><blockquote name="68b6" id="68b6" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong">Path Based Compositional Embeddings&nbsp;:</strong></blockquote><p name="cf1c" id="cf1c" class="graf graf--p graf-after--blockquote">Each function in composition is determined based on a unique set of equivalence classes from each partition,<strong class="markup--strong markup--p-strong"> yielding a unique ‘path’ of transformations.</strong></p><p name="8054" id="8054" class="graf graf--p graf-after--p">Path Based Compositional Embeddings are expected to give better results with the benefit of lower model complexity.</p><h3 name="d0aa" id="d0aa" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">TRADE-OFF&nbsp;:</strong></h3><p name="1897" id="1897" class="graf graf--p graf-after--h3">There’s a catch.</p><ul class="postList"><li name="b1f3" id="b1f3" class="graf graf--li graf-after--p">Larger Embedding table will yield better model quality; but at the cost of increased memory requirements.</li><li name="c93c" id="c93c" class="graf graf--li graf-after--li">Using a more aggressive version will yield smaller models, but lead to a reduction in model quality.</li><li name="d64a" id="d64a" class="graf graf--li graf-after--li">Most models exponentially decrease in performance with a number of parameters.</li><li name="14ef" id="14ef" class="graf graf--li graf-after--li">Both types of compositional embeddings reduce the number of parameters by implicitly enforcing some <strong class="markup--strong markup--li-strong">structure defined by</strong> <strong class="markup--strong markup--li-strong">complementary partitions</strong> in generation of each categories’ embedding.</li><li name="5155" id="5155" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Quality
 of model ought to depend on how closely the chosen partitions reflect 
intrinsic properties of category set and their respective embeddings.</strong></li></ul><h3 name="ad61" id="ad61" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">3. COMMUNICATION BASED&nbsp;:</strong></h3><p name="aead" id="aead" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">( Interaction between Features )</strong></p><p name="f600" id="f600" class="graf graf--p graf-after--p">Source&nbsp;: <a href="https://github.com/thumbe3/Distributed_Training_of_DLRM/blob/master/CS744_group10.pdf" data-href="https://github.com/thumbe3/Distributed_Training_of_DLRM/blob/master/CS744_group10.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/thumbe3/Distributed_Training_of_DLRM/blob/master/CS744_group10.pdf</a></p><figure name="d361" id="d361" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*7_dPgZbjlTAxoAH0" data-width="960" data-height="540" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/07_dPgZbjlTAxoAH0.png"></figure><p name="cc06" id="cc06" class="graf graf--p graf-after--figure">DLRM uses <strong class="markup--strong markup--p-strong">model parallelism </strong>to avoid replicating the whole set of embedding tables on every GPU device and <strong class="markup--strong markup--p-strong">data parallelism</strong> to enable concurrent processing of samples in FC layers.</p><p name="1ae8" id="1ae8" class="graf graf--p graf-after--p">MLP parameters are replicated across GPU devices and not shuffled.</p><p name="4f43" id="4f43" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What is the problem&nbsp;?</strong></p><p name="7e87" id="7e87" class="graf graf--p graf-after--p">Transferring embedding tables across nodes in a cluster becomes expensive and could be a Bottleneck.</p><p name="227a" id="227a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Solution&nbsp;:</strong></p><p name="8d7f" id="8d7f" class="graf graf--p graf-after--p">Since
 it is the interaction between pairs of learned embedding vectors that 
matters and not the absolute values of embedding themselves.</p><p name="4833" id="4833" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">We hypothesize we can learn embeddings in different nodes independently to result in a good model.</em></p><p name="cac4" id="cac4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Saves
 Network Bandwidth by synchronizing only MLP parameters and learning 
Embedding tables independently on each of the server nodes.</strong></p><p name="7e74" id="7e74" class="graf graf--p graf-after--p">In order to speed up training, <strong class="markup--strong markup--p-strong">sharding </strong>of input dataset <strong class="markup--strong markup--p-strong">across cluster nodes </strong>has been implemented such that both nodes can <strong class="markup--strong markup--p-strong">process different shards of data concurrently and therefore do more progress than a single node.</strong></p><figure name="c628" id="c628" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*KXFcMO8GtuQfYhUj" data-width="357" data-height="329" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0KXFcMO8GtuQfYhUj.jpeg"><figcaption class="imageCaption">DISTRIBUTED DLRM</figcaption></figure><p name="8566" id="8566" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Master node collects gradients of MLP parameters</strong> from the slave node and itself. <strong class="markup--strong markup--p-strong">MLP parameters were synchronized</strong> by monitoring their values for some of the experiments.</p><p name="6ff2" id="6ff2" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Embedding tables</em> learnt were different in both nodes as these are not synchronized and nodes work on different shards of input dataset.</p><p name="e6c9" id="e6c9" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Since the DLRM uses Interaction of Embeddings</em> rather than embedding themselves, <em class="markup--em markup--p-em">good models were achievable </em>even though embeddings were not synchronized across the nodes.</p><h3 name="f09e" id="f09e" class="graf graf--h3 graf-after--p">4. COMPUTE DOMINATED&nbsp;:</h3><p name="540e" id="540e" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">(Compute/Run-Time Bottleneck)</strong></p><p name="fdb4" id="fdb4" class="graf graf--p graf-after--p">Source&nbsp;: <a href="https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM" data-href="https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM</a></p><p name="cda0" id="cda0" class="graf graf--p graf-after--p"><a href="https://engineering.fb.com/ml-applications/fbgemm/" data-href="https://engineering.fb.com/ml-applications/fbgemm/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://engineering.fb.com/ml-applications/fbgemm/</a></p><figure name="a9ed" id="a9ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*p7aMU0nxGffa0uDc" data-width="960" data-height="540" src="Deep%20Learning%20Recommendation%20Models%20(DLRM)%20A%20Deep%20Dive_files/0p7aMU0nxGffa0uDc.png"></figure><p name="c8fa" id="c8fa" class="graf graf--p graf-after--figure">As discussed above&nbsp;,</p><ul class="postList"><li name="1000" id="1000" class="graf graf--li graf-after--p">MLP also results in Compute Overload</li><li name="7cba" id="7cba" class="graf graf--li graf-after--li">Co-location
 creates performance bottlenecks when running production-scale 
recommendation models leading to lower resource utilization</li></ul><p name="348e" id="348e" class="graf graf--p graf-after--li">Co-location impacts more on <a href="https://caffe2.ai/docs/operators-catalogue.html" data-href="https://caffe2.ai/docs/operators-catalogue.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">SparseLengthSum</a> due to higher irregular memory accesses, which exhibits less cache reuse.</p><p name="2679" id="2679" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">SOLUTION&nbsp;: FBGEMM (Facebook + General Matrix Multiplication)</strong></p><h4 name="8baa" id="8baa" class="graf graf--h4 graf-after--p">Introducing the workhorse of our&nbsp;model.</h4><p name="bcef" id="bcef" class="graf graf--p graf-after--h4">It is the definite back-end of PyTorch for quantized inference on servers.</p><ul class="postList"><li name="e1ae" id="e1ae" class="graf graf--li graf-after--p">It
 is specifically optimized for low-precision data, unlike the 
conventional linear algebra libraries used in scientific computing 
(which work with FP32 or FP64 precision).</li><li name="4733" id="4733" class="graf graf--li graf-after--li">It
 provides efficient low-precision general matrix-matrix multiplication 
(GEMM) for small batch sizes and support for accuracy-loss-minimizing 
techniques such as row-wise quantization and outlier-aware quantization.</li><li name="a4af" id="a4af" class="graf graf--li graf-after--li">It
 also exploits fusion opportunities to overcome the unique challenges of
 matrix multiplication at lower precision with bandwidth-bound pre- and 
post-GEMM operations.</li></ul><p name="0616" id="0616" class="graf graf--p graf-after--li">A number of improvements to the existing features as well as new features were added in the <a href="https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM" data-href="https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">January 2020 release</a>.</p><p name="11e4" id="11e4" class="graf graf--p graf-after--p">These include Embedding Kernels <em class="markup--em markup--p-em">(very important to us) </em>JIT’ed sparse kernels, and int64 GEMM for Privacy Preserving Machine Learning Models.</p><p name="c8f5" id="c8f5" class="graf graf--p graf-after--p">A couple of Implementation stats&nbsp;:</p><ol class="postList"><li name="083d" id="083d" class="graf graf--li graf-after--p">Reduces DRAM Badnwidth usage in Recommendation Systems by 40%</li><li name="d766" id="d766" class="graf graf--li graf-after--li">Speeds up character detection by 2.4x in <a href="https://engineering.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/" data-href="https://engineering.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Rosetta</a> (ML Algo for detecting text in Images and Videos)</li></ol><p name="fa71" id="fa71" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Computations occur on 64-bit Matrix Multiplication Operations</strong> which is widely used in Privacy-Preserving field, <strong class="markup--strong markup--p-strong">essentially speeding up Privacy Preserving Machine Learning Models.</strong></p><p name="ef28" id="ef28" class="graf graf--p graf-after--p">Currently there exists no good high-performance implementation of 64-bit GEMMs on current generation of CPUs.</p><p name="c0f9" id="c0f9" class="graf graf--p graf-after--p">Therefore&nbsp;,64-bit
 GEMMs has been added to FBGEMM&nbsp;. It achieves 10.5 GOPs/sec on 
Intel Xeon Gold 6138 processor with turbo off. It is 3.5x faster than 
the existing implementation that runs at 3 GOps/sec. This is the first 
iteration of the 64-bit GEMM implementation.</p><p name="b247" id="b247" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">REFERENCES&nbsp;:</em></strong></p><ol class="postList"><li name="279a" id="279a" class="graf graf--li graf-after--p">Recommendation series by James Le: It’s really good for building up basics on Recommendation Systems. <a href="https://jameskle.com/writes/rec-sys-part-1" data-href="https://jameskle.com/writes/rec-sys-part-1" class="markup--anchor markup--li-anchor" rel="nofollow noopener noopener" target="_blank">https://jameskle.com/writes/rec-sys-part-1</a></li><li name="9b92" id="9b92" class="graf graf--li graf-after--li">Deep Learning Recommendation Model forPersonalization and Recommendation Systems <a href="https://arxiv.org/pdf/1906.00091.pdf" data-href="https://arxiv.org/pdf/1906.00091.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1906.00091.pdf</a></li><li name="995b" id="995b" class="graf graf--li graf-after--li">Compositional Embeddings Using Complementary Partitionsfor Memory-Efficient Recommendation Systems <a href="https://arxiv.org/pdf/1909.02107.pdf" data-href="https://arxiv.org/pdf/1909.02107.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1909.02107.pdf</a></li><li name="fb04" id="fb04" class="graf graf--li graf-after--li">On the Dimensionality of Embeddings for Sparse Features and Data <a href="https://arxiv.org/pdf/1901.02103.pdf" data-href="https://arxiv.org/pdf/1901.02103.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1901.02103.pdf</a></li><li name="bc74" id="bc74" class="graf graf--li graf-after--li">The Architectural Implications of Facebook’sDNN-based Personalized Recommendation <a href="https://arxiv.org/pdf/1906.03109.pdf" data-href="https://arxiv.org/pdf/1906.03109.pdf" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://arxiv.org/pdf/1906.03109.pdf</a></li><li name="6842" id="6842" class="graf graf--li graf-after--li"><a href="https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM" data-href="https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://github.com/pytorch/FBGEMM/wiki/Recent-feature-additions-and-improvements-in-FBGEMM</a></li></ol><p name="1327" id="1327" class="graf graf--p graf-after--li">7. Open-sourcing FBGEMM for state-of-the-art server-side inference</p><div name="45c9" id="45c9" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://engineering.fb.com/ml-applications/fbgemm/" data-href="https://engineering.fb.com/ml-applications/fbgemm/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://engineering.fb.com/ml-applications/fbgemm/"><strong class="markup--strong markup--mixtapeEmbed-strong">Open-sourcing FBGEMM for server-side inference - Facebook Engineering</strong><br><em class="markup--em markup--mixtapeEmbed-em">Facebook is open-sourcing FBGEMM, a high-performance kernel library, optimized for server-side inference. Unlike other…</em>engineering.fb.com</a><a href="https://engineering.fb.com/ml-applications/fbgemm/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="26671578c8576cee8027c8ab6ce06d42" data-thumbnail-img-id="0*vzAWIt_r0fSUjR2v" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*vzAWIt_r0fSUjR2v);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@nishantkumar94" class="p-author h-card">Nishant Kumar</a> on <a href="https://medium.com/p/f38a95f47c2c"><time class="dt-published" datetime="2020-10-15T16:35:02.237Z">October 15, 2020</time></a>.</p><p><a href="https://medium.com/@nishantkumar94/deep-learning-recommendation-models-dlrm-a-deep-dive-f38a95f47c2c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com/">Medium</a> on December 6, 2020.</p></footer></article></body></html>